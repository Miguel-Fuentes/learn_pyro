{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVI Part 1: An introduction to Stochastic Variational Inference\n",
    "This is taken from [this](https://pyro.ai/examples/svi_part_i.html) page from the pyro examples section\n",
    "Miguel Fuentes\n",
    "Created: 4/29/2020\n",
    "Last Updated: 4/30/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.distributions.constraints as constraints\n",
    "\n",
    "import pyro\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "import pyro.distributions as dist\n",
    "\n",
    "pyro.set_rng_seed(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We can perform SVI on more or less arbitrary stochastic functions with Pyro. Besides the inputs the main components of a pyro model are as follows:\n",
    "1) Observations (included with pyro.sample using obs keyword)  \n",
    "2) Latent variables (included with pyro.sample)  \n",
    "3) Paramaters (included with pyro.param)   \n",
    "\n",
    "Every set of paramaters defines a joint probability over the observations and the latent variables. To perform SVI we need these assumptions about the joint pdfs defined by the paramaters:  \n",
    "- We can sample from the pdfs\n",
    "- We can compute the pointwise log pdf at any point\n",
    "- The pdf is differentiable w.r.t. the paramaters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What exactly are we trying to learn?\n",
    "We want to find the most likely paramaters for our model, this can be rewritten as \n",
    "$$\\theta_{max} = \\underset{\\theta}{\\text{argmax}} log(p_{\\theta}(x))$$  \n",
    "To compute this quantity we must integrate over the latent variables. Doing this is often intractible, and even if we can do it we usually end up with a really hard non-convex optimization problem.  \n",
    "Additionally, we also want to compute posteriors for the latent variables once we have the most likely paramaters. This requires another challenging computation:  \n",
    "$$p_{\\theta_{max}}(z|x) = \\frac{p_{\\theta_{max}}(x,z)}{\\int d\\textbf{z}p_{\\theta_{max}}(x,z)}$$  \n",
    "We don't want to, of often can't, do these calculations so we need a better way. Variational inference gives us a scheme to calculate $\\theta_{max}$ and getting an approximate estimate for $p_{\\theta_{max}}(z|x)$. For this we need a few things, one of the most important is a guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guide\n",
    "The idea here is to introduce a family of distributions paramaterizes by $\\phi$, $q_{\\phi}(z)$ over the latent variables. We will search this distribution space and try to find the best possible approximation of $p_{\\theta_{max}}(z|x)$. In the literature qe call $\\phi$ the variational paramaters and we call $q_{\\phi}(z)$ the variational distribution. In pyro, this is called the guide because that is shorter and easier to remember.  \n",
    "We will define our guide function the same way we would define any other model in pyro. However, ince we need the guide to produce a joint distribution over the latent variables, we need to impose some constraints:  \n",
    "1) The model and guide should have the same call signature (args and kwargs)  \n",
    "2) The guide should not include any observations\n",
    "3) Any latent variable which is appears in the model (with a pyro.sample call) must also appear in the guide  \n",
    "Once we have defined the guide we can go on to search the distribution space for the best posterior approximation. To do this we need an objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELBO\n",
    "asdasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some data with 6 observed heads and 4 observed tails\n",
    "data = []\n",
    "for _ in range(6):\n",
    "    data.append(torch.tensor(1.0))\n",
    "for _ in range(4):\n",
    "    data.append(torch.tensor(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the param store in case we're in a REPL\n",
    "pyro.clear_param_store()\n",
    "\n",
    "def model(data):\n",
    "    # define the hyperparameters that control the beta prior\n",
    "    alpha0 = torch.tensor(10.0)\n",
    "    beta0 = torch.tensor(10.0)\n",
    "    # sample f from the beta prior\n",
    "    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
    "    # loop over the observed data\n",
    "    for i in range(len(data)):\n",
    "        # observe datapoint i using the bernoulli likelihood\n",
    "        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])\n",
    "\n",
    "def guide(data):\n",
    "    # register the two variational parameters with Pyro\n",
    "    # - both parameters will have initial value 15.0.\n",
    "    # - because we invoke constraints.positive, the optimizer\n",
    "    # will take gradients on the unconstrained parameters\n",
    "    # (which are related to the constrained parameters by a log)\n",
    "    alpha_q = pyro.param(\"alpha_q\", torch.tensor(15.0),\n",
    "                         constraint=constraints.positive)\n",
    "    beta_q = pyro.param(\"beta_q\", torch.tensor(15.0),\n",
    "                        constraint=constraints.positive)\n",
    "    # sample latent_fairness from the distribution Beta(alpha_q, beta_q)\n",
    "    pyro.sample(\"latent_fairness\", dist.Beta(alpha_q, beta_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1500/1500 [00:05<00:00, 297.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "based on the data and our prior belief, the fairness of the coin is 0.535 +- 0.090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# setup the optimizer\n",
    "adam_params = {\"lr\": 0.0005, \"betas\": (0.90, 0.999)}\n",
    "optimizer = Adam(adam_params)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "n_steps = 1500\n",
    "# do gradient steps\n",
    "for step in tqdm(range(n_steps)):\n",
    "    svi.step(data)\n",
    "\n",
    "# grab the learned variational parameters\n",
    "alpha_q = pyro.param(\"alpha_q\").item()\n",
    "beta_q = pyro.param(\"beta_q\").item()\n",
    "\n",
    "# here we use some facts about the beta distribution\n",
    "# compute the inferred mean of the coin's fairness\n",
    "inferred_mean = alpha_q / (alpha_q + beta_q)\n",
    "# compute inferred standard deviation\n",
    "factor = beta_q / (alpha_q * (1.0 + alpha_q + beta_q))\n",
    "inferred_std = inferred_mean * math.sqrt(factor)\n",
    "\n",
    "print(\"\\nbased on the data and our prior belief, the fairness \" +\n",
    "      \"of the coin is %.3f +- %.3f\" % (inferred_mean, inferred_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
